{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30553,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:58:25.806279Z","iopub.execute_input":"2025-05-04T21:58:25.806732Z","iopub.status.idle":"2025-05-04T21:58:25.829696Z","shell.execute_reply.started":"2025-05-04T21:58:25.806677Z","shell.execute_reply":"2025-05-04T21:58:25.828875Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n/kaggle/input/optiver-trading-at-the-close/train.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n/kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:58:25.832176Z","iopub.execute_input":"2025-05-04T21:58:25.832576Z","iopub.status.idle":"2025-05-04T21:58:25.837391Z","shell.execute_reply.started":"2025-05-04T21:58:25.832539Z","shell.execute_reply":"2025-05-04T21:58:25.836253Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ntrain_df = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')\ntrain_df.head()\n#train_df=train_org.sample(frac=0.01, random_state=42)  # 1% sample\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:58:25.838738Z","iopub.execute_input":"2025-05-04T21:58:25.839082Z","iopub.status.idle":"2025-05-04T21:58:41.536610Z","shell.execute_reply.started":"2025-05-04T21:58:25.839050Z","shell.execute_reply":"2025-05-04T21:58:41.535683Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n0         0        0                  0      3180602.69   \n1         1        0                  0       166603.91   \n2         2        0                  0       302879.87   \n3         3        0                  0     11917682.27   \n4         4        0                  0       447549.96   \n\n   imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n0                        1         0.999812   13380276.64        NaN   \n1                       -1         0.999896    1642214.25        NaN   \n2                       -1         0.999561    1819368.03        NaN   \n3                       -1         1.000171   18389745.62        NaN   \n4                       -1         0.999532   17860614.95        NaN   \n\n   near_price  bid_price  bid_size  ask_price   ask_size  wap    target  \\\n0         NaN   0.999812  60651.50   1.000026    8493.03  1.0 -3.029704   \n1         NaN   0.999896   3233.04   1.000660   20605.09  1.0 -5.519986   \n2         NaN   0.999403  37956.00   1.000298   18995.00  1.0 -8.389950   \n3         NaN   0.999999   2324.90   1.000214  479032.40  1.0 -4.010200   \n4         NaN   0.999394  16485.54   1.000016     434.10  1.0 -7.349849   \n\n   time_id row_id  \n0        0  0_0_0  \n1        0  0_0_1  \n2        0  0_0_2  \n3        0  0_0_3  \n4        0  0_0_4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock_id</th>\n      <th>date_id</th>\n      <th>seconds_in_bucket</th>\n      <th>imbalance_size</th>\n      <th>imbalance_buy_sell_flag</th>\n      <th>reference_price</th>\n      <th>matched_size</th>\n      <th>far_price</th>\n      <th>near_price</th>\n      <th>bid_price</th>\n      <th>bid_size</th>\n      <th>ask_price</th>\n      <th>ask_size</th>\n      <th>wap</th>\n      <th>target</th>\n      <th>time_id</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3180602.69</td>\n      <td>1</td>\n      <td>0.999812</td>\n      <td>13380276.64</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999812</td>\n      <td>60651.50</td>\n      <td>1.000026</td>\n      <td>8493.03</td>\n      <td>1.0</td>\n      <td>-3.029704</td>\n      <td>0</td>\n      <td>0_0_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>166603.91</td>\n      <td>-1</td>\n      <td>0.999896</td>\n      <td>1642214.25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999896</td>\n      <td>3233.04</td>\n      <td>1.000660</td>\n      <td>20605.09</td>\n      <td>1.0</td>\n      <td>-5.519986</td>\n      <td>0</td>\n      <td>0_0_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>302879.87</td>\n      <td>-1</td>\n      <td>0.999561</td>\n      <td>1819368.03</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999403</td>\n      <td>37956.00</td>\n      <td>1.000298</td>\n      <td>18995.00</td>\n      <td>1.0</td>\n      <td>-8.389950</td>\n      <td>0</td>\n      <td>0_0_2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11917682.27</td>\n      <td>-1</td>\n      <td>1.000171</td>\n      <td>18389745.62</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999999</td>\n      <td>2324.90</td>\n      <td>1.000214</td>\n      <td>479032.40</td>\n      <td>1.0</td>\n      <td>-4.010200</td>\n      <td>0</td>\n      <td>0_0_3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>447549.96</td>\n      <td>-1</td>\n      <td>0.999532</td>\n      <td>17860614.95</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999394</td>\n      <td>16485.54</td>\n      <td>1.000016</td>\n      <td>434.10</td>\n      <td>1.0</td>\n      <td>-7.349849</td>\n      <td>0</td>\n      <td>0_0_4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=1):\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    print(f\"\\nInitial memory usage: {start_mem:.2f} MB\")\n\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object and not pd.api.types.is_categorical_dtype(col_type):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            old_dtype = df[col].dtype\n\n            if pd.api.types.is_integer_dtype(col_type):\n                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n\n            else:  # floats\n                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n            if verbose and old_dtype != df[col].dtype:\n                print(f\"Column '{col}': {old_dtype} → {df[col].dtype}\")\n\n        elif col_type == object:\n            num_unique = df[col].nunique()\n            num_total = len(df[col])\n            if num_unique / num_total < 0.5:\n                df[col] = df[col].astype('category')\n                if verbose:\n                    print(f\"Column '{col}': object → category\")\n\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    print(f\"\\nFinal memory usage: {end_mem:.2f} MB\")\n    print(f\"Reduced by {(100 * (start_mem - end_mem) / start_mem):.2f}%\")\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:58:41.537915Z","iopub.execute_input":"2025-05-04T21:58:41.538187Z","iopub.status.idle":"2025-05-04T21:58:41.550658Z","shell.execute_reply.started":"2025-05-04T21:58:41.538165Z","shell.execute_reply":"2025-05-04T21:58:41.549531Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def build_metrics(train_df):\n    # Microstructure features from order book\n    train_df['spread'] = train_df['ask_price'] - train_df['bid_price']\n    \n    train_df['mid_price'] = (train_df['ask_price'] + train_df['bid_price']) / 2\n    \n    train_df['wap_diff'] = train_df['wap'] - train_df['mid_price']\n    \n    # Avoid division by zero in imbalance calculation\n    train_df['order_flow_imbalance'] = (\n        (train_df['bid_size'] - train_df['ask_size']) /\n        (train_df['bid_size'] + train_df['ask_size']).replace(0, 1e-6)\n    )\n    # Relative imbalance (magnitude of excess demand/supply relative to matched volume)\n    train_df['relative_imbalance'] = train_df['imbalance_size'] / (train_df['matched_size'] + 1e-6)\n\n    # Binary indicator for buy-side pressure in the auction\n    train_df['is_buy_pressure'] = (train_df['imbalance_buy_sell_flag'] == 1).astype(int)\n\n    # Auction signal strength (how far auction price is from current WAP)\n    train_df['auction_signal_strength'] = train_df['reference_price'] - train_df['wap']\n    # 1-minute = 6 rows (10s intervals), use min_periods=1 to preserve early rows\n    train_df['rolling_avg_imbalance'] = (\n        train_df\n        .groupby(['stock_id', 'date_id'])['imbalance_size']\n        .transform(lambda x: x.rolling(window=6, min_periods=1).mean())\n    )\n    train_df['seconds_to_close'] = 540 - train_df['seconds_in_bucket']\n    train_df['spread_change'] = train_df.groupby(['stock_id', 'date_id'])['spread'].diff()\n    # 10-second WAP velocity (standard row-to-row diff)\n    train_df['wap_velocity'] = (\n        train_df\n        .groupby(['stock_id', 'date_id'])['wap']\n        .diff()\n    )\n\n    # 60-second WAP velocity (lag of 6 rows = 60 seconds in 10s interval data)\n    train_df['wap_velocity_60s'] = (\n        train_df\n        .groupby(['stock_id', 'date_id'])['wap']\n        .diff(periods=6)\n    )\n    # WAP 60 seconds earlier\n    train_df['wap_lag_60s'] = (\n        train_df\n        .groupby(['stock_id', 'date_id'])['wap']\n        .shift(6)\n    )\n    \n    # Spread 60 seconds earlier\n    train_df['spread_lag_60s'] = (\n        train_df\n        .groupby(['stock_id', 'date_id'])['spread']\n        .shift(6)\n    )\n\n    # Order imbalance 60 seconds earlier\n    train_df['imbalance_lag_60s'] = (\n    train_df\n    .groupby(['stock_id', 'date_id'])['imbalance_size']\n    .shift(6)\n    )\n    # Ensure synthetic_index_wap is already computed\n    train_df['synthetic_index_wap'] = (\n        train_df.groupby(['date_id', 'seconds_in_bucket'])['wap'].transform('mean')\n    )\n    \n    #\n    # You can repeat this pattern for any feature you've already created:\n    train_df['wap_velocity_lag_60s'] = train_df.groupby(['stock_id', 'date_id'])['wap_velocity'].shift(6)\n    train_df['spread_change_lag_60s'] = train_df.groupby(['stock_id', 'date_id'])['spread_change'].shift(6)\n    # Now compute the ratio\n    train_df['stock_vs_index_wap_ratio'] = train_df['wap'] / (train_df['synthetic_index_wap'] + 1e-6)\n    bins = [0, 300, 480, 600]\n    labels = ['0_300', '300_480', '480_600']    \n    train_df['window_label'] = pd.cut(\n        train_df['seconds_in_bucket'],\n        bins=bins,\n        labels=labels,\n        right=False  # means 0 <= x < 300, 300 <= x < 400, etc.\n    )\n\n    train_df = reduce_mem_usage(train_df)\n    return train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T22:04:20.886673Z","iopub.execute_input":"2025-05-04T22:04:20.887020Z","iopub.status.idle":"2025-05-04T22:04:20.897769Z","shell.execute_reply.started":"2025-05-04T22:04:20.886993Z","shell.execute_reply":"2025-05-04T22:04:20.896607Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def basic_agg_metrics(group_level,df,common_list,metric_list):\n    column_list=[]\n    for key, value in group_level.items():\n        for metric_name in metric_list:\n            for agg_func in common_list:\n                new_col = metric_name+'_'+key+'_'+agg_func\n                df[new_col] = (\n                    df.groupby(value)[metric_name]\n                      .expanding()\n                     .agg(agg_func)\n                      .reset_index(level=value, drop=True)\n                )\n    df = reduce_mem_usage(df)\n    return df\n    \ndef delta_beg_metrics(df,group_level,metric_list):\n    for key, value in group_level.items():\n        print(key)\n        df = df.sort_values(['stock_id', 'date_id','window_label','seconds_in_bucket']).copy()\n        for metric_name in metric_list:\n            new_col = metric_name+'_'+key\n            if key=='sw':\n                # Window beg and initial\n                df['window_end_'+new_col]=(df.groupby(value, observed=True)[metric_name].transform('last'))\n                df['window_beg_'+new_col]=(df.groupby(value, observed=True)[metric_name].transform('first'))\n                # delta\n                df['delta_within_window'+new_col]=df[metric_name]-df['window_beg_'+new_col]\n            else:\n                df['initial_0s_'+metric_name]=(df.groupby(value, observed=True)[metric_name].transform('first'))\n                df['lag_'+new_col] = (df.groupby(value, observed=True)[metric_name].shift(1))\n                df['delta_'+new_col]=df[metric_name]-df['initial_0s_'+metric_name] \n        df = reduce_mem_usage(df)\n        df = df.copy()\n    return df        \n    \ndef rolling_mean(df, window_size, metric_list):\n    df = df.sort_values(['stock_id', 'seconds_in_bucket', 'date_id'])\n\n    for feat in metric_list:\n        rolcol=str(feat)+'_rollmean_'+str(window_size)\n        df[rolcol] = (\n            df.groupby(['stock_id', 'seconds_in_bucket'])[feat]\n              .transform(lambda x: x.shift(1).rolling(window=window_size, min_periods=1).mean())\n        )\n        df[rolcol] = df[rolcol].fillna(df[feat])\n        df = reduce_mem_usage(df)\n        df = df.copy()\n    return df\n\ndef window_agg(df,metric_list):\n    # Aggregations to apply\n    agg_funcs = ['min', 'max', 'mean', 'std'] \n    # Apply groupby with aggregation\n    df_window_agg = df.groupby(['stock_id','date_id','window_label'],observed=True)[metric_list].agg(agg_funcs)\n    df_window_agg.columns = ['_'.join(col) for col in df_window_agg.columns]\n    df_window_agg = df_window_agg.reset_index()\n    agg_cols=[]\n    for i in metric_list:\n        for j in agg_funcs:\n            agg_cols.append(i+'_'+j)\n    pivot_data = df_window_agg.pivot(index=['stock_id','date_id'], columns='window_label', values=agg_cols)\n    pivot_data.columns = ['_'.join(col) for col in pivot_data.columns]\n    pivot_data = pivot_data.reset_index()\n    return pivot_data\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T22:04:22.531635Z","iopub.execute_input":"2025-05-04T22:04:22.532270Z","iopub.status.idle":"2025-05-04T22:04:22.544068Z","shell.execute_reply.started":"2025-05-04T22:04:22.532242Z","shell.execute_reply":"2025-05-04T22:04:22.543202Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def na_imputation(df):\n    # Step 1: Sort so that forward fill respects time order\n    df = df.sort_values(['stock_id', 'date_id', 'seconds_in_bucket'])\n    cols_with_na = df.columns[df.isna().any()].tolist()\n    print(cols_with_na)\n    # Step 2: Forward fill missing prices per stock and date\n    for col in cols_with_na:\n        #df['missing'+str(col)] = df[col].isna().astype(int)  # optional missing flag\n        df[col] = df.groupby(['stock_id', 'date_id'])[col].ffill()\n    for col in df.columns:\n        if 'std' in col:\n            #df[f'{col}_was_missing'] = df[col].isna().astype(int)  # optional flag\n            df[col] = df[col].fillna(0)  # fill std NaNs with 0  \n   # df['lag_near_price_sd'] = df['lag_near_price_sd'].fillna(df['near_price'])\n    df = reduce_mem_usage(df)\n    df = df.copy()\n    \n    return df\n\ndef window_0_300_data(df):\n    df_1= df[df['window_label']=='0_300'] \n    cols_df1=['stock_vs_index_wap_ratio','near_price','spread','relative_imbalance','far_price']\n    cols_to_drop = [col for col in df_1.columns if any(substr in col for substr in cols_df1)]+['time_id','row_id','window_label']\n    cols_df2 = ['window_end']\n        # Drop columns that contain substrings in cols_df1 but NOT '0_300'\n    cols_to_drop2 = [col for col in df_1.columns \n                        if any(substr in col for substr in cols_df2) ]\n    cols_to_drop_fin=cols_to_drop+cols_to_drop2\n    df_1.drop(columns=cols_to_drop_fin, inplace=True)\n    df_1 = df_1.copy()\n    df_1 = reduce_mem_usage(df_1)\n    return df_1\n    \ndef window_300_480(df,pivot_data,cols_to_merge_1,merge_keys):\n    if '300_480' in df['window_label'].values:\n        df_2= df[df['window_label']=='300_480'].copy()\n        df_2 = reduce_mem_usage(df_2)\n        df_2 = df_2.merge(pivot_data[merge_keys + cols_to_merge_1], on=merge_keys, how='left')\n        #Dropping cols which aren't neccessary\n        cols_df2=['bid_size','order_flow_imbalance','matched_size','relative_imbalance']\n        cols_to_drop = [col for col in df_2.columns if any(substr in col for substr in cols_df2)]\n        cols_df2 = ['window_end']\n        # Drop columns that contain substrings in cols_df1 but NOT '0_300'\n        cols_to_drop2 = [col for col in df_2.columns \n                        if any(substr in col for substr in cols_df2) and '0_300' not in col]\n        cols_to_drop1 = [col for col in df_2.columns \n                        if any(substr in col for substr in ['near_price']) and '0_300' in col]\n        cols_to_drop_fin=cols_to_drop1+cols_to_drop2+cols_to_drop+['time_id','row_id','window_label']\n        df_2 = df_2.drop(columns=cols_to_drop_fin)\n        df_2 = reduce_mem_usage(df_2) \n        df_2 =df_2.copy()\n    else:\n        df_2 = pd.DataFrame(columns=df.columns.union(cols_to_merge_1))  # Or skip entirely\n\n    return df_2\n\n\n\n\ndef window_480_600(df,pivot_data,cols_to_merge_1,cols_to_merge_2,merge_keys):\n    if '480_600' in df['window_label'].values:\n        df_3= df[df['window_label']=='480_600'].copy()\n        df_3 = reduce_mem_usage(df_3)  \n        df_3 = df_3.merge(pivot_data[merge_keys + cols_to_merge_1], on=merge_keys, how='left')\n        df_3 = df_3.merge(pivot_data[merge_keys + cols_to_merge_2], on=merge_keys, how='left')\n        #Dropping cols which aren't neccessary\n        cols_df3=['order_flow_imbalance','bid_size','matched_size','relative_imbalance']\n        cols_to_drop = [col for col in df_3.columns if any(substr in col for substr in cols_df3)]\n        cols_df2 = ['window_end']\n        cols_to_drop2 = [col for col in df_3.columns \n                        if any(substr in col for substr in cols_df2) and '0_300' not in col]\n        cols_to_drop3 = [col for col in df_3.columns \n                        if any(substr in col for substr in cols_df2) and '300_480' not in col]\n        \n        cols_to_drop1 = [col for col in df_3.columns \n                        if any(substr in col for substr in ['near_price']) and '0_300' in col]\n        cols_to_drop_fin=cols_to_drop1+cols_to_drop2+cols_to_drop+['time_id','row_id','window_label']\n        df_3 = df_3.drop(columns=cols_to_drop_fin)\n        df_3 = reduce_mem_usage(df_3) \n        df_3 =df_3.copy()\n    else:\n        df_3 = pd.DataFrame(columns=df.columns.union(cols_to_merge_2))  # Or skip entirely\n    return df_3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T22:17:02.313577Z","iopub.execute_input":"2025-05-04T22:17:02.313899Z","iopub.status.idle":"2025-05-04T22:17:02.329491Z","shell.execute_reply.started":"2025-05-04T22:17:02.313872Z","shell.execute_reply":"2025-05-04T22:17:02.328418Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"metric_list=['wap_diff','order_flow_imbalance','auction_signal_strength','bid_size','matched_size','stock_vs_index_wap_ratio','near_price','spread','relative_imbalance']\n\ndef full_preprocessing_pipeline(data,metric_list, is_test=True):\n    \n    # 1. Add domain-specific features\n    df = build_metrics(data) \n    df=df.sort_values(['stock_id', 'date_id', 'seconds_in_bucket']).copy()\n    if not is_test:\n        df=df[~(df['stock_id'].astype(int).isin([19,101,158,131]))].copy()\n        \n    # 2. Building basic metrics like min max avg and index metric to showcase variation across all stocks\n    group_level_basic={'sd':['stock_id', 'date_id'],'sw':['stock_id', 'date_id','window_label'],'all':['date_id']}\n    common_list=['mean']\n    df=basic_agg_metrics(group_level_basic,df,common_list,metric_list)\n\n    # 3. Building basic metrics like delta and window beg window end etc..\n    #group_level={'sd':['stock_id', 'date_id'],'sw':['stock_id', 'date_id','window_label']}\n    #df=delta_beg_metrics(df,group_level,metric_list)\n\n    # 4. Rolling Mean across X days \n    #df=rolling_mean(df, window_size=10, metric_list=metric_list)\n    \n    # 5. Window level agg data\n    pivot_df=window_agg(df,metric_list)\n\n    # 6. Creating df for 0-300 window \n    df_1=window_0_300_data(df) \n\n    # 7. Null Imputation \n    df=na_imputation(df)\n\n    #Historical window metrics\n    merge_keys = ['stock_id', 'date_id']\n    cols_to_merge_1 = [col for col in pivot_df.columns if '0_300' in col]\n    cols_to_merge_2 = [col for col in pivot_df.columns if '300_480' in col]\n    \n    # 8. Creating df for 300-480 window \n    df_2=window_300_480(df,pivot_df,cols_to_merge_1,merge_keys)\n\n    # 9. Creating df for 480-600 window \n    df_3=window_480_600(df,pivot_df,cols_to_merge_1,cols_to_merge_2,merge_keys)\n    return df_1,df_2,df_3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T22:22:30.854038Z","iopub.execute_input":"2025-05-04T22:22:30.854351Z","iopub.status.idle":"2025-05-04T22:22:30.862004Z","shell.execute_reply.started":"2025-05-04T22:22:30.854324Z","shell.execute_reply":"2025-05-04T22:22:30.861067Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"metric_list=['wap_diff','order_flow_imbalance','auction_signal_strength','bid_size','matched_size','stock_vs_index_wap_ratio','near_price','spread','relative_imbalance']\ntrain_0_300, train_300_480, train_480_600 = full_preprocessing_pipeline(train_df,metric_list,is_test=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T22:22:38.901072Z","iopub.execute_input":"2025-05-04T22:22:38.901443Z","iopub.status.idle":"2025-05-04T22:25:59.683707Z","shell.execute_reply.started":"2025-05-04T22:22:38.901390Z","shell.execute_reply":"2025-05-04T22:25:59.682833Z"}},"outputs":[{"name":"stdout","text":"\nInitial memory usage: 894.28 MB\nColumn 'order_flow_imbalance': float64 → float16\nColumn 'relative_imbalance': float64 → float16\nColumn 'is_buy_pressure': int64 → int8\nColumn 'rolling_avg_imbalance': float64 → float32\nColumn 'synthetic_index_wap': float32 → float16\nColumn 'stock_vs_index_wap_ratio': float64 → float16\n\nFinal memory usage: 739.42 MB\nReduced by 17.32%\n\nInitial memory usage: 1820.80 MB\nColumn 'wap_diff_sd_mean': float64 → float16\nColumn 'order_flow_imbalance_sd_mean': float64 → float16\nColumn 'auction_signal_strength_sd_mean': float64 → float16\nColumn 'bid_size_sd_mean': float64 → float32\nColumn 'matched_size_sd_mean': float64 → float32\nColumn 'stock_vs_index_wap_ratio_sd_mean': float64 → float16\nColumn 'near_price_sd_mean': float64 → float16\nColumn 'spread_sd_mean': float64 → float16\nColumn 'relative_imbalance_sd_mean': float64 → float16\nColumn 'wap_diff_sw_mean': float64 → float16\nColumn 'order_flow_imbalance_sw_mean': float64 → float16\nColumn 'auction_signal_strength_sw_mean': float64 → float16\nColumn 'bid_size_sw_mean': float64 → float32\nColumn 'matched_size_sw_mean': float64 → float32\nColumn 'stock_vs_index_wap_ratio_sw_mean': float64 → float16\nColumn 'near_price_sw_mean': float64 → float16\nColumn 'spread_sw_mean': float64 → float16\nColumn 'relative_imbalance_sw_mean': float64 → float16\nColumn 'wap_diff_all_mean': float64 → float16\nColumn 'order_flow_imbalance_all_mean': float64 → float16\nColumn 'auction_signal_strength_all_mean': float64 → float16\nColumn 'bid_size_all_mean': float64 → float32\nColumn 'matched_size_all_mean': float64 → float32\nColumn 'stock_vs_index_wap_ratio_all_mean': float64 → float16\nColumn 'near_price_all_mean': float64 → float16\nColumn 'spread_all_mean': float64 → float16\nColumn 'relative_imbalance_all_mean': float64 → float16\n\nFinal memory usage: 1086.64 MB\nReduced by 40.32%\n\nInitial memory usage: 299.00 MB\n\nFinal memory usage: 299.00 MB\nReduced by 0.00%\n['far_price', 'near_price', 'spread_change', 'wap_velocity', 'wap_velocity_60s', 'wap_lag_60s', 'spread_lag_60s', 'imbalance_lag_60s', 'wap_velocity_lag_60s', 'spread_change_lag_60s', 'near_price_sd_mean', 'near_price_sw_mean', 'near_price_all_mean']\n\nInitial memory usage: 1086.64 MB\n\nFinal memory usage: 1086.64 MB\nReduced by 0.00%\n\nInitial memory usage: 355.95 MB\n\nFinal memory usage: 355.95 MB\nReduced by 0.00%\n\nInitial memory usage: 358.80 MB\nColumn 'wap_diff_min_0_300': float64 → float16\nColumn 'wap_diff_max_0_300': float64 → float16\nColumn 'wap_diff_mean_0_300': float64 → float16\nColumn 'wap_diff_std_0_300': float64 → float16\nColumn 'auction_signal_strength_min_0_300': float64 → float16\nColumn 'auction_signal_strength_max_0_300': float64 → float16\nColumn 'auction_signal_strength_mean_0_300': float64 → float16\nColumn 'auction_signal_strength_std_0_300': float64 → float16\nColumn 'stock_vs_index_wap_ratio_min_0_300': float64 → float16\nColumn 'stock_vs_index_wap_ratio_max_0_300': float64 → float16\nColumn 'stock_vs_index_wap_ratio_mean_0_300': float64 → float16\nColumn 'stock_vs_index_wap_ratio_std_0_300': float64 → float16\nColumn 'spread_min_0_300': float64 → float16\nColumn 'spread_max_0_300': float64 → float16\nColumn 'spread_mean_0_300': float64 → float16\nColumn 'spread_std_0_300': float64 → float16\n\nFinal memory usage: 205.03 MB\nReduced by 42.86%\n\nInitial memory usage: 138.42 MB\nColumn 'seconds_to_close': int16 → int8\n\nFinal memory usage: 137.80 MB\nReduced by 0.45%\n\nInitial memory usage: 238.58 MB\nColumn 'wap_diff_min_0_300': float64 → float16\nColumn 'wap_diff_max_0_300': float64 → float16\nColumn 'wap_diff_mean_0_300': float64 → float16\nColumn 'wap_diff_std_0_300': float64 → float16\nColumn 'auction_signal_strength_min_0_300': float64 → float16\nColumn 'auction_signal_strength_max_0_300': float64 → float16\nColumn 'auction_signal_strength_mean_0_300': float64 → float16\nColumn 'auction_signal_strength_std_0_300': float64 → float16\nColumn 'stock_vs_index_wap_ratio_min_0_300': float64 → float16\nColumn 'stock_vs_index_wap_ratio_max_0_300': float64 → float16\nColumn 'stock_vs_index_wap_ratio_mean_0_300': float64 → float16\nColumn 'stock_vs_index_wap_ratio_std_0_300': float64 → float16\nColumn 'spread_min_0_300': float64 → float16\nColumn 'spread_max_0_300': float64 → float16\nColumn 'spread_mean_0_300': float64 → float16\nColumn 'spread_std_0_300': float64 → float16\nColumn 'wap_diff_min_300_480': float64 → float16\nColumn 'wap_diff_max_300_480': float64 → float16\nColumn 'wap_diff_mean_300_480': float64 → float16\nColumn 'wap_diff_std_300_480': float64 → float16\nColumn 'auction_signal_strength_min_300_480': float64 → float16\nColumn 'auction_signal_strength_max_300_480': float64 → float16\nColumn 'auction_signal_strength_mean_300_480': float64 → float16\nColumn 'auction_signal_strength_std_300_480': float64 → float16\nColumn 'stock_vs_index_wap_ratio_min_300_480': float64 → float16\nColumn 'stock_vs_index_wap_ratio_max_300_480': float64 → float16\nColumn 'stock_vs_index_wap_ratio_mean_300_480': float64 → float16\nColumn 'stock_vs_index_wap_ratio_std_300_480': float64 → float16\nColumn 'near_price_min_300_480': float64 → float16\nColumn 'near_price_max_300_480': float64 → float16\nColumn 'near_price_mean_300_480': float64 → float16\nColumn 'near_price_std_300_480': float64 → float16\nColumn 'spread_min_300_480': float64 → float16\nColumn 'spread_max_300_480': float64 → float16\nColumn 'spread_mean_300_480': float64 → float16\nColumn 'spread_std_300_480': float64 → float16\n\nFinal memory usage: 104.03 MB\nReduced by 56.40%\n","output_type":"stream"}],"execution_count":18}]}