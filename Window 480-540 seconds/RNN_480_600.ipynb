{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLdfrsggE04B",
        "outputId": "b712d384-7ea4-4ef6-b724-11a705e3da0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/df_3 (2).parquet'\n",
        "\n",
        "try:\n",
        "  df = pd.read_parquet(file_path)\n",
        "  print(df.head()) # Display first few rows of the dataframe\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: File not found at {file_path}\")\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9Wg7BfsFLT4",
        "outputId": "c32558b5-ec25-4ca1-8fda-1adea385870a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
            "0         0        0                480      1268787.96   \n",
            "1         0        0                490      1180033.46   \n",
            "2         0        0                500      1158754.78   \n",
            "3         0        0                510      1088759.13   \n",
            "4         0        0                520      1008684.09   \n",
            "\n",
            "   imbalance_buy_sell_flag  reference_price  far_price  near_price  bid_price  \\\n",
            "0                        1         0.999491   1.000241    1.000241   0.999383   \n",
            "1                        1         0.999383   1.000241    1.000026   0.999276   \n",
            "2                        1         0.999491   1.000241    1.000026   0.999383   \n",
            "3                        1         0.999598   1.000241    1.000241   0.999491   \n",
            "4                        1         0.999598   1.000241    1.000026   0.999383   \n",
            "\n",
            "   ask_price  ...  stock_vs_index_wap_ratio_mean_300_480  \\\n",
            "0   0.999491  ...                                0.99939   \n",
            "1   0.999383  ...                                0.99939   \n",
            "2   0.999491  ...                                0.99939   \n",
            "3   0.999598  ...                                0.99939   \n",
            "4   0.999598  ...                                0.99939   \n",
            "\n",
            "   stock_vs_index_wap_ratio_std_300_480  near_price_min_300_480  \\\n",
            "0                              0.000354                0.998955   \n",
            "1                              0.000354                0.998955   \n",
            "2                              0.000354                0.998955   \n",
            "3                              0.000354                0.998955   \n",
            "4                              0.000354                0.998955   \n",
            "\n",
            "   near_price_max_300_480 near_price_mean_300_480  near_price_std_300_480  \\\n",
            "0                1.000241                0.999437                0.000398   \n",
            "1                1.000241                0.999437                0.000398   \n",
            "2                1.000241                0.999437                0.000398   \n",
            "3                1.000241                0.999437                0.000398   \n",
            "4                1.000241                0.999437                0.000398   \n",
            "\n",
            "   spread_min_300_480  spread_max_300_480  spread_mean_300_480  \\\n",
            "0            0.000107            0.000215             0.000125   \n",
            "1            0.000107            0.000215             0.000125   \n",
            "2            0.000107            0.000215             0.000125   \n",
            "3            0.000107            0.000215             0.000125   \n",
            "4            0.000107            0.000215             0.000125   \n",
            "\n",
            "   spread_std_300_480  \n",
            "0            0.000041  \n",
            "1            0.000041  \n",
            "2            0.000041  \n",
            "3            0.000041  \n",
            "4            0.000041  \n",
            "\n",
            "[5 rows x 160 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Special value for masking\n",
        "SPECIAL_VALUE = -999.0\n",
        "df['far_price'] = df.groupby('stock_id')['far_price'].ffill()\n",
        "df['far_price'] = df['far_price'].fillna(1)"
      ],
      "metadata": {
        "id": "per1wF1bKyRO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Find columns with nans and number\n",
        "\n",
        "# Find columns with NaNs and their counts\n",
        "nan_counts = df.isna().sum()\n",
        "nan_cols = nan_counts[nan_counts > 0]\n",
        "\n",
        "print(\"Columns with NaN values:\\n\", nan_cols)\n",
        "\n",
        "# Number of columns with NaNs\n",
        "num_nan_cols = len(nan_cols)\n",
        "print(\"\\nNumber of columns with NaN values:\", num_nan_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoIy27w2OEdW",
        "outputId": "3fd4fb89-168d-4b6b-befe-467797347c19"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns with NaN values:\n",
            " Series([], dtype: int64)\n",
            "\n",
            "Number of columns with NaN values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets\n",
        "train_df = df[df['date_id'] <= 400].copy()\n",
        "test_df = df[df['date_id'] > 400].copy()\n",
        "\n",
        "# Print summary\n",
        "print(f\"Train days: {train_df['date_id'].min()} to {train_df['date_id'].max()} -> {train_df.shape}\")\n",
        "print(f\"Test days: {test_df['date_id'].min()} to {test_df['date_id'].max()} -> {test_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYG-kO-CFXo1",
        "outputId": "3954d405-2601-4040-fd51-e2009efd8acd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train days: 0 to 400 -> (543424, 160)\n",
            "Test days: 401 to 480 -> (109760, 160)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['seconds_in_bucket'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj25BDEiJPDr",
        "outputId": "2d402a6e-0335-444d-9e34-31f70e3b605e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([480, 490, 500, 510, 520, 530, 540])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ----- 1. Separate features & target -----\n",
        "target_col = 'target'  # Replace with your actual target\n",
        "exclude_cols = ['date_id', 'time_id', 'target', 'seconds_in_bucket']  # Add others if needed\n",
        "\n",
        "# All columns except target & known categorical/time identifiers\n",
        "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
        "\n",
        "# ----- 2. Identify numeric & categorical -----\n",
        "categorical_cols = ['stock_id']\n",
        "do_not_scale = ['imbalance_buy_sell_flag']\n",
        "numeric_cols = [col for col in feature_cols if col not in categorical_cols]\n",
        "scale_cols = [col for col in numeric_cols if col not in do_not_scale]\n",
        "\n",
        "# ----- 3. Scale numeric features -----\n",
        "scaler = StandardScaler()\n",
        "train_df[scale_cols] = scaler.fit_transform(train_df[scale_cols])\n",
        "test_df[scale_cols] = scaler.transform(test_df[scale_cols])\n",
        "\n",
        "# for col in scale_cols:\n",
        "#     valid_train = train_df[col] != SPECIAL_VALUE\n",
        "#     valid_test = test_df[col] != SPECIAL_VALUE\n",
        "\n",
        "#     scaled_train = scaler.fit_transform(train_df.loc[valid_train, [col]]).astype(np.float32).flatten()\n",
        "#     scaled_test = scaler.transform(test_df.loc[valid_test, [col]]).astype(np.float32).flatten()\n",
        "\n",
        "#     # Ensure the column is float first to prevent assignment issues\n",
        "#     train_df[col] = train_df[col].astype(np.float32)\n",
        "#     test_df[col] = test_df[col].astype(np.float32)\n",
        "\n",
        "#     train_df.loc[valid_train, col] = scaled_train\n",
        "#     test_df.loc[valid_test, col] = scaled_test\n"
      ],
      "metadata": {
        "id": "TrOgFiprF2E_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.copy()\n",
        "test_df = test_df.copy()"
      ],
      "metadata": {
        "id": "9-4-37P5F6uE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def reshape_to_tensor(df, feature_cols, target_col='target', timesteps_per_day=7):\n",
        "    \"\"\"\n",
        "    Converts a long-form DataFrame into (X, y) tensors for GRU sequence-to-sequence learning.\n",
        "\n",
        "    Args:\n",
        "        df: pandas DataFrame with 'stock_id', 'date_id', 'seconds_in_bucket', features, and target\n",
        "        feature_cols: list of column names to be used as input features\n",
        "        target_col: name of the column to use as target\n",
        "        timesteps_per_day: number of time intervals per day (default: 30)\n",
        "\n",
        "    Returns:\n",
        "        X: np.array of shape (N_samples, 30, num_features)\n",
        "        y: np.array of shape (N_samples, 30)\n",
        "        keys: list of tuples (stock_id, date_id) representing each sample\n",
        "    \"\"\"\n",
        "    grouped = df.groupby(['stock_id', 'date_id'])\n",
        "    X_list, y_list, keys = [], [], []\n",
        "\n",
        "    for (stock_id, date_id), group in grouped:\n",
        "        group_sorted = group.sort_values('seconds_in_bucket')\n",
        "        if len(group_sorted) != timesteps_per_day:\n",
        "            # print(stock_id)\n",
        "            continue  # skip incomplete days\n",
        "\n",
        "        X_seq = group_sorted[feature_cols].values  # shape: (30, F)\n",
        "        y_seq = group_sorted[target_col].values    # shape: (30,)\n",
        "\n",
        "        X_list.append(X_seq)\n",
        "        y_list.append(y_seq)\n",
        "        keys.append((stock_id, date_id))\n",
        "\n",
        "    X = np.stack(X_list)  # shape: (N, 30, F)\n",
        "    y = np.stack(y_list)  # shape: (N, 30)\n",
        "    return X, y, keys\n"
      ],
      "metadata": {
        "id": "c9X3hESXF-Ix"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_y, train_keys = reshape_to_tensor(train_df, feature_cols)\n",
        "test_X, test_y, test_keys = reshape_to_tensor(test_df, feature_cols)"
      ],
      "metadata": {
        "id": "Ww8ptGckGGu4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape\n",
        "# test_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eWPqc-mGKMl",
        "outputId": "e5366ce8-21d9-412b-8139-7df7742f78ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(77632, 7, 156)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape\n",
        "test_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9rT4ChOHk8u",
        "outputId": "fea2668b-2714-4015-d16f-00ebd23bf33e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15680, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import TimeDistributed, Masking\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "-ZihN-G5GOS8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Time-Based Cross-Validation Split Function ---\n",
        "def time_based_cv(df, group_col='date_id', n_splits=5):\n",
        "    unique_dates = sorted(df[group_col].unique())\n",
        "    split_size = len(unique_dates) // (n_splits + 1)\n",
        "    for i in range(1, n_splits + 1):\n",
        "        train_days = unique_dates[: i * split_size]\n",
        "        test_days = unique_dates[i * split_size : (i + 1) * split_size]\n",
        "        train_idx = df[df[group_col].isin(train_days)].index\n",
        "        test_idx = df[df[group_col].isin(test_days)].index\n",
        "        yield train_idx, test_idx\n",
        "\n",
        "def build_gru_model(n_layers=1, input_shape=(7, 156), dropout=0.2):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "    for i in range(n_layers):\n",
        "        return_seq = True  # we want return_sequences=True in all layers\n",
        "        model.add(GRU(64, return_sequences=return_seq))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(TimeDistributed(Dense(1)))\n",
        "    model.compile(optimizer='adam', loss='mae')\n",
        "    return model"
      ],
      "metadata": {
        "id": "cHmkPWJnGUpw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hyperparameter Search ---\n",
        "def run_time_cv_gru(train_X, train_y, df_train_keys, layer_choices=[1, 2, 3, 4]):\n",
        "    results = {}\n",
        "    for n_layers in layer_choices:\n",
        "        print(f\"\\nTraining model with {n_layers} GRU layer(s)...\")\n",
        "        fold = 0\n",
        "        fold_mae = []\n",
        "        for train_idx, val_idx in time_based_cv(df_train_keys, group_col='date_id', n_splits=5):\n",
        "            fold += 1\n",
        "            print(f\"\\nFold {fold}\")\n",
        "            X_train, X_val = train_X[train_idx], train_X[val_idx]\n",
        "            y_train, y_val = train_y[train_idx], train_y[val_idx]\n",
        "\n",
        "            model = build_gru_model(n_layers=n_layers, input_shape=(train_X.shape[1], train_X.shape[2]))\n",
        "            es = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "            model.fit(X_train, y_train,\n",
        "                      validation_data=(X_val, y_val),\n",
        "                      epochs=100,\n",
        "                      batch_size=32,\n",
        "                      callbacks=[es],\n",
        "                      verbose=1)\n",
        "\n",
        "            preds = model.predict(X_val)\n",
        "            mae = mean_absolute_error(y_val.flatten(), preds.flatten())\n",
        "            print(f\"Fold {fold} MAE: {mae:.5f}\")\n",
        "            fold_mae.append(mae)\n",
        "\n",
        "        results[n_layers] = np.mean(fold_mae)\n",
        "        print(f\"Average MAE for {n_layers} layer(s): {results[n_layers]:.5f}\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "F1tE45FVGXce"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_keys_df = pd.DataFrame(train_keys, columns=[\"stock_id\", \"date_id\"])"
      ],
      "metadata": {
        "id": "kXqCq4uhGcOL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqvMNaUYR1El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_time_cv_gru(train_X, train_y, train_keys_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k84-joQRGeBe",
        "outputId": "b157006c-0c48-4f87-d6f5-88a936cdd950"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with 1 GRU layer(s)...\n",
            "\n",
            "Fold 1\n",
            "Epoch 1/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 4.8820 - val_loss: 6.3649\n",
            "Epoch 2/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.8065 - val_loss: 6.3421\n",
            "Epoch 3/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.7510 - val_loss: 6.3309\n",
            "Epoch 4/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.6989 - val_loss: 6.3643\n",
            "Epoch 5/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.7746 - val_loss: 6.3281\n",
            "Epoch 6/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.7041 - val_loss: 6.3219\n",
            "Epoch 7/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.7039 - val_loss: 6.3214\n",
            "Epoch 8/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.6451 - val_loss: 6.3282\n",
            "Epoch 9/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.6491 - val_loss: 6.3396\n",
            "Epoch 10/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.6422 - val_loss: 6.3489\n",
            "Epoch 11/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.6518 - val_loss: 6.3736\n",
            "Epoch 12/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.5557 - val_loss: 6.3599\n",
            "Epoch 13/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.5260 - val_loss: 6.3767\n",
            "Epoch 14/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.5217 - val_loss: 6.3872\n",
            "Epoch 15/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.5157 - val_loss: 6.3776\n",
            "Epoch 16/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 4.5138 - val_loss: 6.3997\n",
            "Epoch 17/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.4103 - val_loss: 6.4213\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 1 MAE: 6.32138\n",
            "\n",
            "Fold 2\n",
            "Epoch 1/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 5.6542 - val_loss: 6.4125\n",
            "Epoch 2/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.5680 - val_loss: 6.3901\n",
            "Epoch 3/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.5016 - val_loss: 6.4301\n",
            "Epoch 4/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.4667 - val_loss: 6.3699\n",
            "Epoch 5/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.4969 - val_loss: 6.3956\n",
            "Epoch 6/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.4150 - val_loss: 6.3895\n",
            "Epoch 7/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.4359 - val_loss: 6.4069\n",
            "Epoch 8/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.3815 - val_loss: 6.3910\n",
            "Epoch 9/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.3778 - val_loss: 6.4030\n",
            "Epoch 10/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.3651 - val_loss: 6.4338\n",
            "Epoch 11/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.3699 - val_loss: 6.4313\n",
            "Epoch 12/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.3402 - val_loss: 6.4568\n",
            "Epoch 13/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.2536 - val_loss: 6.4557\n",
            "Epoch 14/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 5.2492 - val_loss: 6.4487\n",
            "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2 MAE: 6.36985\n",
            "\n",
            "Fold 3\n",
            "Epoch 1/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.9538 - val_loss: 5.7304\n",
            "Epoch 2/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.8237 - val_loss: 5.7342\n",
            "Epoch 3/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.8092 - val_loss: 5.7376\n",
            "Epoch 4/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.7582 - val_loss: 5.7188\n",
            "Epoch 5/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.7634 - val_loss: 5.7758\n",
            "Epoch 6/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.7439 - val_loss: 5.7407\n",
            "Epoch 7/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.6830 - val_loss: 5.7474\n",
            "Epoch 8/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.6645 - val_loss: 5.7520\n",
            "Epoch 9/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.6529 - val_loss: 5.7609\n",
            "Epoch 10/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.6233 - val_loss: 5.7668\n",
            "Epoch 11/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.6291 - val_loss: 5.7782\n",
            "Epoch 12/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.6156 - val_loss: 5.7840\n",
            "Epoch 13/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.5448 - val_loss: 5.7930\n",
            "Epoch 14/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.5336 - val_loss: 5.7829\n",
            "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3 MAE: 5.71880\n",
            "\n",
            "Fold 4\n",
            "Epoch 1/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.9304 - val_loss: 5.7264\n",
            "Epoch 2/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.8157 - val_loss: 5.7066\n",
            "Epoch 3/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.7948 - val_loss: 5.7022\n",
            "Epoch 4/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.7580 - val_loss: 5.6994\n",
            "Epoch 5/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.7322 - val_loss: 5.6848\n",
            "Epoch 6/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.7313 - val_loss: 5.7052\n",
            "Epoch 7/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.7418 - val_loss: 5.6898\n",
            "Epoch 8/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.6933 - val_loss: 5.7060\n",
            "Epoch 9/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.6130 - val_loss: 5.7051\n",
            "Epoch 10/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.6192 - val_loss: 5.6983\n",
            "Epoch 11/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.6111 - val_loss: 5.7137\n",
            "Epoch 12/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.5756 - val_loss: 5.7063\n",
            "Epoch 13/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.6024 - val_loss: 5.7369\n",
            "Epoch 14/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.5274 - val_loss: 5.7124\n",
            "Epoch 15/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.5780 - val_loss: 5.7471\n",
            "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 4 MAE: 5.68477\n",
            "\n",
            "Fold 5\n",
            "Epoch 1/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 5.8985 - val_loss: 5.3879\n",
            "Epoch 2/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.7949 - val_loss: 5.3657\n",
            "Epoch 3/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.7543 - val_loss: 5.3626\n",
            "Epoch 4/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.7268 - val_loss: 5.3666\n",
            "Epoch 5/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.6678 - val_loss: 5.3985\n",
            "Epoch 6/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.7051 - val_loss: 5.3872\n",
            "Epoch 7/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.6472 - val_loss: 5.3809\n",
            "Epoch 8/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.6748 - val_loss: 5.3949\n",
            "Epoch 9/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.6556 - val_loss: 5.3878\n",
            "Epoch 10/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.6796 - val_loss: 5.4057\n",
            "Epoch 11/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.6529 - val_loss: 5.4101\n",
            "Epoch 12/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 5.6330 - val_loss: 5.4395\n",
            "Epoch 13/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.5863 - val_loss: 5.4252\n",
            "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 5 MAE: 5.36262\n",
            "Average MAE for 1 layer(s): 5.89148\n",
            "\n",
            "Training model with 2 GRU layer(s)...\n",
            "\n",
            "Fold 1\n",
            "Epoch 1/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 4.8953 - val_loss: 6.4032\n",
            "Epoch 2/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.9154 - val_loss: 6.3516\n",
            "Epoch 3/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.8238 - val_loss: 6.3351\n",
            "Epoch 4/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.7331 - val_loss: 6.3354\n",
            "Epoch 5/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.6809 - val_loss: 6.3298\n",
            "Epoch 6/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.6775 - val_loss: 6.3417\n",
            "Epoch 7/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.6941 - val_loss: 6.3413\n",
            "Epoch 8/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.7495 - val_loss: 6.3616\n",
            "Epoch 9/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.6856 - val_loss: 6.3769\n",
            "Epoch 10/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.6036 - val_loss: 6.4190\n",
            "Epoch 11/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.6045 - val_loss: 6.3874\n",
            "Epoch 12/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.5891 - val_loss: 6.3908\n",
            "Epoch 13/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.5469 - val_loss: 6.4524\n",
            "Epoch 14/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.5171 - val_loss: 6.4626\n",
            "Epoch 15/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.5580 - val_loss: 6.4713\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 1 MAE: 6.32977\n",
            "\n",
            "Fold 2\n",
            "Epoch 1/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 5.6769 - val_loss: 6.4236\n",
            "Epoch 2/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 5.5922 - val_loss: 6.3989\n",
            "Epoch 3/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 5.5480 - val_loss: 6.3997\n",
            "Epoch 4/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 5.4808 - val_loss: 6.4051\n",
            "Epoch 5/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 5.4742 - val_loss: 6.4042\n",
            "Epoch 6/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 5.4513 - val_loss: 6.4399\n",
            "Epoch 7/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 5.4686 - val_loss: 6.4215\n",
            "Epoch 8/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 5.3594 - val_loss: 6.4767\n",
            "Epoch 9/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 5.3615 - val_loss: 6.4427\n",
            "Epoch 10/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 5.3633 - val_loss: 6.5037\n",
            "Epoch 11/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 5.3594 - val_loss: 6.4623\n",
            "Epoch 12/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 5.2750 - val_loss: 6.5405\n",
            "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2 MAE: 6.39888\n",
            "\n",
            "Fold 3\n",
            "Epoch 1/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 5.9261 - val_loss: 5.7400\n",
            "Epoch 2/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.8587 - val_loss: 5.7270\n",
            "Epoch 3/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.7985 - val_loss: 5.7355\n",
            "Epoch 4/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.7446 - val_loss: 5.7293\n",
            "Epoch 5/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.7777 - val_loss: 5.7422\n",
            "Epoch 6/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.7841 - val_loss: 5.7771\n",
            "Epoch 7/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.6553 - val_loss: 5.7337\n",
            "Epoch 8/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.6541 - val_loss: 5.8026\n",
            "Epoch 9/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.6204 - val_loss: 5.7729\n",
            "Epoch 10/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.6110 - val_loss: 5.8118\n",
            "Epoch 11/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.6534 - val_loss: 5.8081\n",
            "Epoch 12/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.5430 - val_loss: 5.7970\n",
            "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3 MAE: 5.72699\n",
            "\n",
            "Fold 4\n",
            "Epoch 1/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 5.8952 - val_loss: 5.7532\n",
            "Epoch 2/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.8034 - val_loss: 5.7065\n",
            "Epoch 3/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.7719 - val_loss: 5.7032\n",
            "Epoch 4/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.7656 - val_loss: 5.7179\n",
            "Epoch 5/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.7416 - val_loss: 5.6985\n",
            "Epoch 6/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.7095 - val_loss: 5.6944\n",
            "Epoch 7/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.6939 - val_loss: 5.7008\n",
            "Epoch 8/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.6763 - val_loss: 5.7031\n",
            "Epoch 9/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.6559 - val_loss: 5.6929\n",
            "Epoch 10/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.6666 - val_loss: 5.7314\n",
            "Epoch 11/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.6393 - val_loss: 5.6942\n",
            "Epoch 12/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.5581 - val_loss: 5.7271\n",
            "Epoch 13/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.5465 - val_loss: 5.7237\n",
            "Epoch 14/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.5644 - val_loss: 5.7653\n",
            "Epoch 15/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.5377 - val_loss: 5.7319\n",
            "Epoch 16/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.5266 - val_loss: 5.7522\n",
            "Epoch 17/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.5334 - val_loss: 5.7308\n",
            "Epoch 18/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.5376 - val_loss: 5.7407\n",
            "Epoch 19/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 5.4620 - val_loss: 5.7556\n",
            "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 4 MAE: 5.69289\n",
            "\n",
            "Fold 5\n",
            "Epoch 1/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - loss: 5.8694 - val_loss: 5.3813\n",
            "Epoch 2/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 5.8096 - val_loss: 5.3972\n",
            "Epoch 3/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 5.7456 - val_loss: 5.3643\n",
            "Epoch 4/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 5.7260 - val_loss: 5.3710\n",
            "Epoch 5/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 5.7331 - val_loss: 5.3944\n",
            "Epoch 6/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 5.6833 - val_loss: 5.3674\n",
            "Epoch 7/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 5.6936 - val_loss: 5.4239\n",
            "Epoch 8/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 5.6651 - val_loss: 5.3942\n",
            "Epoch 9/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 5.6487 - val_loss: 5.3867\n",
            "Epoch 10/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 5.6484 - val_loss: 5.3966\n",
            "Epoch 11/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 5.6206 - val_loss: 5.3942\n",
            "Epoch 12/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 5.5942 - val_loss: 5.3849\n",
            "Epoch 13/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 5.6201 - val_loss: 5.3883\n",
            "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 5 MAE: 5.36434\n",
            "Average MAE for 2 layer(s): 5.90257\n",
            "\n",
            "Training model with 3 GRU layer(s)...\n",
            "\n",
            "Fold 1\n",
            "Epoch 1/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 4.9754 - val_loss: 6.4209\n",
            "Epoch 2/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.8296 - val_loss: 6.3731\n",
            "Epoch 3/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.7748 - val_loss: 6.3543\n",
            "Epoch 4/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.7864 - val_loss: 6.3377\n",
            "Epoch 5/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.7721 - val_loss: 6.3342\n",
            "Epoch 6/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.6812 - val_loss: 6.3464\n",
            "Epoch 7/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.6468 - val_loss: 6.3561\n",
            "Epoch 8/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.6699 - val_loss: 6.3791\n",
            "Epoch 9/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.6143 - val_loss: 6.3814\n",
            "Epoch 10/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.6418 - val_loss: 6.3813\n",
            "Epoch 11/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.6165 - val_loss: 6.4121\n",
            "Epoch 12/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.5776 - val_loss: 6.4238\n",
            "Epoch 13/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.5592 - val_loss: 6.4855\n",
            "Epoch 14/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.5352 - val_loss: 6.5180\n",
            "Epoch 15/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.5735 - val_loss: 6.4857\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Fold 1 MAE: 6.33424\n",
            "\n",
            "Fold 2\n",
            "Epoch 1/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 5.6642 - val_loss: 6.4338\n",
            "Epoch 2/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 5.5163 - val_loss: 6.4107\n",
            "Epoch 3/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 5.5554 - val_loss: 6.4014\n",
            "Epoch 4/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 5.5551 - val_loss: 6.4102\n",
            "Epoch 5/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 5.4769 - val_loss: 6.4082\n",
            "Epoch 6/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 5.4504 - val_loss: 6.4195\n",
            "Epoch 7/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 5.4539 - val_loss: 6.4525\n",
            "Epoch 8/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 5.4196 - val_loss: 6.4599\n",
            "Epoch 9/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 5.3507 - val_loss: 6.4790\n",
            "Epoch 10/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 5.3775 - val_loss: 6.4576\n",
            "Epoch 11/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 5.3027 - val_loss: 6.4557\n",
            "Epoch 12/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 5.2592 - val_loss: 6.4647\n",
            "Epoch 13/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 5.2848 - val_loss: 6.4744\n",
            "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Fold 2 MAE: 6.40138\n",
            "\n",
            "Fold 3\n",
            "Epoch 1/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - loss: 5.9823 - val_loss: 5.7544\n",
            "Epoch 2/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 5.8407 - val_loss: 5.7308\n",
            "Epoch 3/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 5.8221 - val_loss: 5.7162\n",
            "Epoch 4/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 5.7818 - val_loss: 5.7322\n",
            "Epoch 5/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 5.7796 - val_loss: 5.7663\n",
            "Epoch 6/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 5.7523 - val_loss: 5.7377\n",
            "Epoch 7/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 5.7191 - val_loss: 5.8415\n",
            "Epoch 8/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 5.7108 - val_loss: 5.7537\n",
            "Epoch 9/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 5.7027 - val_loss: 5.7781\n",
            "Epoch 10/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 5.6870 - val_loss: 5.7832\n",
            "Epoch 11/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 5.6306 - val_loss: 5.7814\n",
            "Epoch 12/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 5.6241 - val_loss: 5.7819\n",
            "Epoch 13/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 5.5750 - val_loss: 5.8038\n",
            "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Fold 3 MAE: 5.71620\n",
            "\n",
            "Fold 4\n",
            "Epoch 1/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - loss: 5.9201 - val_loss: 5.7315\n",
            "Epoch 2/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.8482 - val_loss: 5.7166\n",
            "Epoch 3/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.7539 - val_loss: 5.7016\n",
            "Epoch 4/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.7716 - val_loss: 5.6968\n",
            "Epoch 5/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.7679 - val_loss: 5.6979\n",
            "Epoch 6/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.7101 - val_loss: 5.6947\n",
            "Epoch 7/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.6740 - val_loss: 5.6939\n",
            "Epoch 8/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.7165 - val_loss: 5.6913\n",
            "Epoch 9/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.6793 - val_loss: 5.6968\n",
            "Epoch 10/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.6339 - val_loss: 5.7128\n",
            "Epoch 11/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.6205 - val_loss: 5.7034\n",
            "Epoch 12/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.6348 - val_loss: 5.7300\n",
            "Epoch 13/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.6027 - val_loss: 5.7014\n",
            "Epoch 14/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.5651 - val_loss: 5.6953\n",
            "Epoch 15/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - loss: 5.5666 - val_loss: 5.7166\n",
            "Epoch 16/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.5496 - val_loss: 5.7361\n",
            "Epoch 17/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.4881 - val_loss: 5.7533\n",
            "Epoch 18/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5.5033 - val_loss: 5.7675\n",
            "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Fold 4 MAE: 5.69131\n",
            "\n",
            "Fold 5\n",
            "Epoch 1/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - loss: 5.8390 - val_loss: 5.3801\n",
            "Epoch 2/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.7836 - val_loss: 5.3693\n",
            "Epoch 3/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.7449 - val_loss: 5.3870\n",
            "Epoch 4/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.7370 - val_loss: 5.3653\n",
            "Epoch 5/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.7523 - val_loss: 5.3578\n",
            "Epoch 6/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.7224 - val_loss: 5.3528\n",
            "Epoch 7/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - loss: 5.7215 - val_loss: 5.3917\n",
            "Epoch 8/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.6893 - val_loss: 5.4160\n",
            "Epoch 9/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.6450 - val_loss: 5.3644\n",
            "Epoch 10/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.6490 - val_loss: 5.4142\n",
            "Epoch 11/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.6169 - val_loss: 5.3636\n",
            "Epoch 12/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.6419 - val_loss: 5.3811\n",
            "Epoch 13/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.6182 - val_loss: 5.3620\n",
            "Epoch 14/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.5934 - val_loss: 5.3900\n",
            "Epoch 15/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 5.5477 - val_loss: 5.3880\n",
            "Epoch 16/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - loss: 5.5604 - val_loss: 5.4012\n",
            "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Fold 5 MAE: 5.35276\n",
            "Average MAE for 3 layer(s): 5.89918\n",
            "\n",
            "Training model with 4 GRU layer(s)...\n",
            "\n",
            "Fold 1\n",
            "Epoch 1/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - loss: 4.9542 - val_loss: 6.4313\n",
            "Epoch 2/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.9040 - val_loss: 6.3516\n",
            "Epoch 3/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.7830 - val_loss: 6.3284\n",
            "Epoch 4/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.8613 - val_loss: 6.3293\n",
            "Epoch 5/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.7729 - val_loss: 6.3503\n",
            "Epoch 6/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.7148 - val_loss: 6.3529\n",
            "Epoch 7/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.7533 - val_loss: 6.3777\n",
            "Epoch 8/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.7138 - val_loss: 6.4068\n",
            "Epoch 9/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.6792 - val_loss: 6.3850\n",
            "Epoch 10/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.6827 - val_loss: 6.4010\n",
            "Epoch 11/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.6568 - val_loss: 6.4079\n",
            "Epoch 12/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.5909 - val_loss: 6.4642\n",
            "Epoch 13/100\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 4.6635 - val_loss: 6.5195\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "Fold 1 MAE: 6.32839\n",
            "\n",
            "Fold 2\n",
            "Epoch 1/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 5.6546 - val_loss: 6.4576\n",
            "Epoch 2/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.6238 - val_loss: 6.4408\n",
            "Epoch 3/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.4908 - val_loss: 6.4171\n",
            "Epoch 4/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.5051 - val_loss: 6.4315\n",
            "Epoch 5/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.5032 - val_loss: 6.4167\n",
            "Epoch 6/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.4984 - val_loss: 6.4540\n",
            "Epoch 7/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.4447 - val_loss: 6.4566\n",
            "Epoch 8/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.4315 - val_loss: 6.4348\n",
            "Epoch 9/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.4372 - val_loss: 6.4161\n",
            "Epoch 10/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.3403 - val_loss: 6.4239\n",
            "Epoch 11/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.3453 - val_loss: 6.5468\n",
            "Epoch 12/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.3853 - val_loss: 6.4389\n",
            "Epoch 13/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.2871 - val_loss: 6.4588\n",
            "Epoch 14/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.2991 - val_loss: 6.4755\n",
            "Epoch 15/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.3249 - val_loss: 6.5297\n",
            "Epoch 16/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.2615 - val_loss: 6.4851\n",
            "Epoch 17/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.2187 - val_loss: 6.5767\n",
            "Epoch 18/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.1838 - val_loss: 6.5342\n",
            "Epoch 19/100\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.1720 - val_loss: 6.5299\n",
            "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "Fold 2 MAE: 6.41613\n",
            "\n",
            "Fold 3\n",
            "Epoch 1/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - loss: 5.9577 - val_loss: 5.7269\n",
            "Epoch 2/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.8696 - val_loss: 5.7174\n",
            "Epoch 3/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.7849 - val_loss: 5.7221\n",
            "Epoch 4/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.7325 - val_loss: 5.7146\n",
            "Epoch 5/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.7865 - val_loss: 5.7798\n",
            "Epoch 6/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.7632 - val_loss: 5.7917\n",
            "Epoch 7/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.7232 - val_loss: 5.7527\n",
            "Epoch 8/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.7461 - val_loss: 5.7378\n",
            "Epoch 9/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.6761 - val_loss: 5.7915\n",
            "Epoch 10/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.7080 - val_loss: 5.8455\n",
            "Epoch 11/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.6405 - val_loss: 5.7643\n",
            "Epoch 12/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.6770 - val_loss: 5.7675\n",
            "Epoch 13/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.6095 - val_loss: 5.7791\n",
            "Epoch 14/100\n",
            "\u001b[1m1186/1186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.5671 - val_loss: 5.7770\n",
            "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "Fold 3 MAE: 5.71463\n",
            "\n",
            "Fold 4\n",
            "Epoch 1/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 5.9151 - val_loss: 5.7438\n",
            "Epoch 2/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.7857 - val_loss: 5.7247\n",
            "Epoch 3/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.8310 - val_loss: 5.7154\n",
            "Epoch 4/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.7696 - val_loss: 5.6910\n",
            "Epoch 5/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.7673 - val_loss: 5.6932\n",
            "Epoch 6/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.7820 - val_loss: 5.6985\n",
            "Epoch 7/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.7740 - val_loss: 5.7172\n",
            "Epoch 8/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.7010 - val_loss: 5.7038\n",
            "Epoch 9/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.6823 - val_loss: 5.6915\n",
            "Epoch 10/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.6542 - val_loss: 5.7129\n",
            "Epoch 11/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.6655 - val_loss: 5.7154\n",
            "Epoch 12/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.5973 - val_loss: 5.7055\n",
            "Epoch 13/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.6045 - val_loss: 5.7228\n",
            "Epoch 14/100\n",
            "\u001b[1m1588/1588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 5.5869 - val_loss: 5.7341\n",
            "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "Fold 4 MAE: 5.69100\n",
            "\n",
            "Fold 5\n",
            "Epoch 1/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - loss: 5.8711 - val_loss: 5.4024\n",
            "Epoch 2/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.7801 - val_loss: 5.3826\n",
            "Epoch 3/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.7918 - val_loss: 5.3620\n",
            "Epoch 4/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.7513 - val_loss: 5.3766\n",
            "Epoch 5/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.6919 - val_loss: 5.3611\n",
            "Epoch 6/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.7276 - val_loss: 5.3874\n",
            "Epoch 7/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.7021 - val_loss: 5.4396\n",
            "Epoch 8/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.7209 - val_loss: 5.3577\n",
            "Epoch 9/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.6676 - val_loss: 5.3529\n",
            "Epoch 10/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.6517 - val_loss: 5.3905\n",
            "Epoch 11/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.6334 - val_loss: 5.3744\n",
            "Epoch 12/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.6238 - val_loss: 5.3935\n",
            "Epoch 13/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.6302 - val_loss: 5.3725\n",
            "Epoch 14/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.5952 - val_loss: 5.3861\n",
            "Epoch 15/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.5739 - val_loss: 5.3757\n",
            "Epoch 16/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.6138 - val_loss: 5.3943\n",
            "Epoch 17/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.5835 - val_loss: 5.3717\n",
            "Epoch 18/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.5489 - val_loss: 5.3793\n",
            "Epoch 19/100\n",
            "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 5.5529 - val_loss: 5.3946\n",
            "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "Fold 5 MAE: 5.35291\n",
            "Average MAE for 4 layer(s): 5.90061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_layers = min(results, key=results.get)\n",
        "print(f\"Best # of layers: {best_layers}, MAE: {results[best_layers]:.5f}\")\n",
        "\n",
        "# Rebuild and retrain on full data\n",
        "final_model = build_gru_model(n_layers=best_layers, input_shape=(train_X.shape[1], train_X.shape[2]))\n",
        "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "final_model.fit(train_X, train_y,\n",
        "                validation_split=0.1,  # optional small holdout for early stopping\n",
        "                epochs=100,\n",
        "                batch_size=32,\n",
        "                callbacks=[es],\n",
        "                verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxgWt6GLahSl",
        "outputId": "b8c425ed-e047-423d-803b-ebab081c4f5d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best # of layers: 1, MAE: 5.89148\n",
            "Epoch 1/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 5.8621 - val_loss: 5.1133\n",
            "Epoch 2/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.7605 - val_loss: 5.1324\n",
            "Epoch 3/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.7599 - val_loss: 5.0949\n",
            "Epoch 4/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.7232 - val_loss: 5.0885\n",
            "Epoch 5/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.7018 - val_loss: 5.0816\n",
            "Epoch 6/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.6595 - val_loss: 5.0758\n",
            "Epoch 7/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.6692 - val_loss: 5.1404\n",
            "Epoch 8/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.6379 - val_loss: 5.0759\n",
            "Epoch 9/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.6368 - val_loss: 5.0667\n",
            "Epoch 10/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.6419 - val_loss: 5.0573\n",
            "Epoch 11/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.6185 - val_loss: 5.0744\n",
            "Epoch 12/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.6271 - val_loss: 5.0775\n",
            "Epoch 13/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.5825 - val_loss: 5.0684\n",
            "Epoch 14/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.5674 - val_loss: 5.0722\n",
            "Epoch 15/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.5901 - val_loss: 5.1284\n",
            "Epoch 16/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.5287 - val_loss: 5.0601\n",
            "Epoch 17/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.5518 - val_loss: 5.0659\n",
            "Epoch 18/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.5305 - val_loss: 5.0638\n",
            "Epoch 19/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.5477 - val_loss: 5.0822\n",
            "Epoch 20/100\n",
            "\u001b[1m2184/2184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 5.4890 - val_loss: 5.0776\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d453cdc4690>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.save('/content/drive/MyDrive/best_gru_model3.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmWaBFfaankk",
        "outputId": "f68a2936-f485-41d6-8e6a-0096be442dd6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "\n",
        "final_model = tf.keras.models.load_model(\n",
        "    '/content/drive/MyDrive/best_gru_model3.h5',\n",
        "    custom_objects={'mae': MeanAbsoluteError()}\n",
        ")\n",
        "\n",
        "\n",
        "# Make predictions on holdout test set\n",
        "test_preds = final_model.predict(test_X)\n",
        "\n",
        "test_mae = mean_absolute_error(test_y.flatten(), test_preds.flatten())\n",
        "print(f\"Test MAE on holdout: {test_mae:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl-Aq4h8aqnD",
        "outputId": "94de9cc3-9543-49c5-a661-ac6229c69248"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Test MAE on holdout: 4.99451\n"
          ]
        }
      ]
    }
  ]
}